{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-31T15:59:49.590448Z",
     "iopub.status.busy": "2022-03-31T15:59:49.589989Z",
     "iopub.status.idle": "2022-03-31T15:59:53.491809Z",
     "shell.execute_reply": "2022-03-31T15:59:53.491083Z",
     "shell.execute_reply.started": "2022-03-31T15:59:49.590373Z"
    }
   },
   "source": [
    "# BERT embeddings and LSTM model for predicting user rating of hotels based on the review text\n",
    "The dataset was provided by HSE \"Intro to Deep Learning\" [course](http://wiki.cs.hse.ru/Основы_глубинного_обучения).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:34:46.339817Z",
     "iopub.status.busy": "2022-04-13T13:34:46.339429Z",
     "iopub.status.idle": "2022-04-13T13:34:55.963750Z",
     "shell.execute_reply": "2022-04-13T13:34:55.962646Z",
     "shell.execute_reply.started": "2022-04-13T13:34:46.339734Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports and setting up wandb\n",
    "\n",
    "import torch \n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import wandb \n",
    "\n",
    "import transformers as ts\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "TRAIN_PATH = '../input/hotelreviews/train.csv'\n",
    "\n",
    "wandb.login(key='XXX') #placeholder key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset consists of 100.000 reviews, each review consists of two separate parts: positive feedback and negative feedback. The rating is provided in the `score` column - a real number in range of 0-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:34:55.966157Z",
     "iopub.status.busy": "2022-04-13T13:34:55.965913Z",
     "iopub.status.idle": "2022-04-13T13:34:56.492341Z",
     "shell.execute_reply": "2022-04-13T13:34:56.491383Z",
     "shell.execute_reply.started": "2022-04-13T13:34:55.966130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>There were issues with the wifi connection</td>\n",
       "      <td>No Positive</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TV not working</td>\n",
       "      <td>No Positive</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>More pillows</td>\n",
       "      <td>Beautiful room Great location Lovely staff</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very business</td>\n",
       "      <td>Location</td>\n",
       "      <td>5.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rooms could do with a bit of a refurbishment ...</td>\n",
       "      <td>Nice breakfast handy for Victoria train stati...</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hotel is under reconstruction and should be c...</td>\n",
       "      <td>Location is excellent for congress activities</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Noise from the trains and road but ok for one...</td>\n",
       "      <td>Great location to tube station and local shop...</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>No Negative</td>\n",
       "      <td>Great location friendly staff and lovely acco...</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I known you re renovating but having concierg...</td>\n",
       "      <td>Staff were super helpful and friendly Great l...</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Location of room no phone signal</td>\n",
       "      <td>friendly staff</td>\n",
       "      <td>6.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            negative  \\\n",
       "0         There were issues with the wifi connection   \n",
       "1                                     TV not working   \n",
       "2                                       More pillows   \n",
       "3                                      Very business   \n",
       "4   Rooms could do with a bit of a refurbishment ...   \n",
       "5   Hotel is under reconstruction and should be c...   \n",
       "6   Noise from the trains and road but ok for one...   \n",
       "7                                        No Negative   \n",
       "8   I known you re renovating but having concierg...   \n",
       "9                   Location of room no phone signal   \n",
       "\n",
       "                                            positive  score  \n",
       "0                                        No Positive    7.1  \n",
       "1                                        No Positive    7.5  \n",
       "2        Beautiful room Great location Lovely staff    10.0  \n",
       "3                                           Location    5.4  \n",
       "4   Nice breakfast handy for Victoria train stati...    6.7  \n",
       "5     Location is excellent for congress activities     6.3  \n",
       "6   Great location to tube station and local shop...    8.8  \n",
       "7   Great location friendly staff and lovely acco...   10.0  \n",
       "8   Staff were super helpful and friendly Great l...    9.2  \n",
       "9                                     friendly staff    6.7  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = pd.read_csv(TRAIN_PATH).drop(['review_id'], axis=1)\n",
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:34:56.495511Z",
     "iopub.status.busy": "2022-04-13T13:34:56.494925Z",
     "iopub.status.idle": "2022-04-13T13:34:56.503059Z",
     "shell.execute_reply": "2022-04-13T13:34:56.501889Z",
     "shell.execute_reply.started": "2022-04-13T13:34:56.495453Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_all(seed_value): #function to fix random seed\n",
    "    random.seed(seed_value) \n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value) \n",
    "    if torch.cuda.is_available() :\n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value) \n",
    "        torch.backends.cudnn.deterministic = True \n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do a bit of data cleansing, such as removing some bad symbols and converting the text to lowercase. This dataset does not have punctuation included, so we don't have to worry about handling that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:34:56.506849Z",
     "iopub.status.busy": "2022-04-13T13:34:56.506315Z",
     "iopub.status.idle": "2022-04-13T13:34:56.514885Z",
     "shell.execute_reply": "2022-04-13T13:34:56.513919Z",
     "shell.execute_reply.started": "2022-04-13T13:34:56.506778Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text) #non-ASCII\n",
    "    text = re.sub('[\\\\r\\\\t\\\\n]+', ' ', text) #delimiters\n",
    "    return ' '.join(text.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with one text we will concatenate both parts of the review with a special `BERT` separator token `[SEP]` in between. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:34:56.517320Z",
     "iopub.status.busy": "2022-04-13T13:34:56.516784Z",
     "iopub.status.idle": "2022-04-13T13:34:58.849662Z",
     "shell.execute_reply": "2022-04-13T13:34:58.848650Z",
     "shell.execute_reply.started": "2022-04-13T13:34:56.517280Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_data():\n",
    "    df = pd.read_csv(TRAIN_PATH).drop(['review_id'], axis=1)    \n",
    "    df['review'] = df.negative + ' [SEP] ' + df.positive\n",
    "    df.review = df.review.apply(lambda x: clean_text(x))\n",
    "    return train_test_split(df.review, df.score, test_size=0.2, random_state=21)\n",
    "        \n",
    "reviews_train, reviews_val, ratings_train, ratings_val = get_train_val_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining basic `PyTorch` `Dataset`.**\n",
    "\n",
    "Since we will be handling vocabulary and tokenization on our own, there is no point in using `torchtext` `Field` and `TabularDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:34:58.851344Z",
     "iopub.status.busy": "2022-04-13T13:34:58.851026Z",
     "iopub.status.idle": "2022-04-13T13:34:58.858340Z",
     "shell.execute_reply": "2022-04-13T13:34:58.857480Z",
     "shell.execute_reply.started": "2022-04-13T13:34:58.851305Z"
    }
   },
   "outputs": [],
   "source": [
    "class ReviewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, reviews, ratings):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.reviews[idx], self.ratings[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating `PyTorch` `DataLoader` istances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:34:58.860366Z",
     "iopub.status.busy": "2022-04-13T13:34:58.859866Z",
     "iopub.status.idle": "2022-04-13T13:34:58.871644Z",
     "shell.execute_reply": "2022-04-13T13:34:58.870663Z",
     "shell.execute_reply.started": "2022-04-13T13:34:58.860326Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZES = (16, 256)\n",
    "\n",
    "def get_train_val_dataloaders():\n",
    "    train_dataset = ReviewsDataset(reviews_train.values, ratings_train.values)\n",
    "    val_dataset = ReviewsDataset(reviews_val.values, ratings_val.values)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZES[0], shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZES[1], shuffle=False)\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "train_dataloader, val_dataloader = get_train_val_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downloading pretrained `BERT` and `tokenizer`.**\n",
    "\n",
    "Since we'll be using pretrained `BERT`, we will need to use exactly the same `tokenizer` which it has been trained with. This is done by using `ts.AutoTokenizer.from_pretrained` and specifying model name as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:34:58.875571Z",
     "iopub.status.busy": "2022-04-13T13:34:58.875106Z",
     "iopub.status.idle": "2022-04-13T13:35:34.941406Z",
     "shell.execute_reply": "2022-04-13T13:35:34.940445Z",
     "shell.execute_reply.started": "2022-04-13T13:34:58.875530Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "def get_bert_and_tokenizer():\n",
    "    BERT = ts.DistilBertModel.from_pretrained(MODEL_NAME, output_hidden_states=True).to(DEVICE).eval()\n",
    "    TOKENIZER = ts.AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    return BERT, TOKENIZER\n",
    "\n",
    "BERT, TOKENIZER = get_bert_and_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the model.**\n",
    "\n",
    "Instead of using an embedding layer to get embeddings for the text, we'll be using the pretrained `BERT` model. These embeddings will then be fed to the same `LSTM` architecture used in the previous notebook.\n",
    "\n",
    "Within the forward pass, we wrap the transformer in a `no_grad` to ensure no gradients are calculated over this part of the model. \n",
    "\n",
    "Input sequence is tokenized with the following parameters:\n",
    "* `padding='longest'` - this will ensure that all instances in the batch will be padded to the length of a longest instance in the batch. \n",
    "* `truncation=True` - this will ensure that if the sequence is longer than the maximum length of a `BERT` model input then it will be truncated.\n",
    "* `return_tensors='pt'` - this will ensure that the tokenizer outputs `PyTorch` tensors.\n",
    "\n",
    "We retrieve the `input_ids` of internal `tokenizer` vocabulary for every word in a sequence as well as an `attention_mask` - in this case a mask which ignores padded elements - and feed it to the `BERT` model. \n",
    "\n",
    "There are a few different ways to get embedding representation of the input sequence as shown in the original [BERT paper](https://arxiv.org/pdf/1810.04805.pdf). The approach with taking second-to-last hidden state was chosen in this architecture.\n",
    "\n",
    "Finally, a sequence of embeddings is fed to the same `LSTM` architecture used in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:35:34.943816Z",
     "iopub.status.busy": "2022-04-13T13:35:34.943184Z",
     "iopub.status.idle": "2022-04-13T13:35:34.957218Z",
     "shell.execute_reply": "2022-04-13T13:35:34.956290Z",
     "shell.execute_reply.started": "2022-04-13T13:35:34.943773Z"
    }
   },
   "outputs": [],
   "source": [
    "class BERT_LSTM(torch.nn.Module) :\n",
    "    def __init__(self, \n",
    "                 bert, \n",
    "                 tokenizer, \n",
    "                 lstm_hidden_dim=300, \n",
    "                 num_layers=1, \n",
    "                 bidirectional=True,\n",
    "                 dropout=0.3):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=self.bert.config.to_dict()['dim'], \n",
    "                            hidden_size=lstm_hidden_dim, \n",
    "                            bidirectional=bidirectional, \n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.linear = nn.Linear(lstm_hidden_dim * 2, 1)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        with torch.no_grad():\n",
    "            tokens = self.tokenizer(list(text), \n",
    "                                    padding='longest', \n",
    "                                    truncation=True, \n",
    "                                    return_tensors='pt')\n",
    "            \n",
    "            input_ids, attention_mask = tokens['input_ids'], tokens['attention_mask']\n",
    "            input_ids, attention_mask = input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "            \n",
    "            outputs = self.bert(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask)\n",
    "            hidden_state = outputs.hidden_states[-2]\n",
    "        \n",
    "        hidden_state = self.dropout(hidden_state)\n",
    "        lstm_out, (hst, cst) = self.lstm(hidden_state)\n",
    "        lstm_hidden = torch.cat([hst[-2, :, :], hst[-1, :, :]], dim=1)\n",
    "        return self.linear(lstm_hidden).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the model and moving it to GPU if available.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:35:34.962609Z",
     "iopub.status.busy": "2022-04-13T13:35:34.961540Z",
     "iopub.status.idle": "2022-04-13T13:35:36.220479Z",
     "shell.execute_reply": "2022-04-13T13:35:36.219317Z",
     "shell.execute_reply.started": "2022-04-13T13:35:34.962552Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_all(21)\n",
    "\n",
    "HIDDEN_DIM = 300\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.3\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "bert_lstm = BERT_LSTM(\n",
    "    bert=BERT,\n",
    "    tokenizer=TOKENIZER,\n",
    "    lstm_hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    bidirectional=BIDIRECTIONAL,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "bert_lstm = bert_lstm.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Freezing the `BERT` part of the model since we are only interested in embeddings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:35:36.224781Z",
     "iopub.status.busy": "2022-04-13T13:35:36.224481Z",
     "iopub.status.idle": "2022-04-13T13:35:36.232272Z",
     "shell.execute_reply": "2022-04-13T13:35:36.231216Z",
     "shell.execute_reply.started": "2022-04-13T13:35:36.224714Z"
    }
   },
   "outputs": [],
   "source": [
    "def freeze_bert():\n",
    "    for name, param in bert_lstm.named_parameters():                \n",
    "        if name.startswith('bert'):\n",
    "            param.requires_grad = False\n",
    "            \n",
    "freeze_bert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining learning rate, optimizer, scheduler and loss function.**   \n",
    "\n",
    "We'll be using MAE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:35:36.234838Z",
     "iopub.status.busy": "2022-04-13T13:35:36.233944Z",
     "iopub.status.idle": "2022-04-13T13:35:36.247017Z",
     "shell.execute_reply": "2022-04-13T13:35:36.245824Z",
     "shell.execute_reply.started": "2022-04-13T13:35:36.234778Z"
    }
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "LOSS_NAME = 'MAE'\n",
    "\n",
    "optimizer = torch.optim.AdamW(bert_lstm.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.98)\n",
    "criterion = F.l1_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the train loop.**\n",
    "\n",
    "`predict` function will return three tensors:  \n",
    "* `losses` tensor with the loss value for every element of iterator  \n",
    "* `predicted_ratings` tensor with the model predictions  \n",
    "* `true_ratings` tensor with the actual ratings\n",
    "\n",
    "We'll also be [exporting](https://wandb.ai/porfiry/hotels-reviews?workspace=user-porfiry) the results to `wandb` to easily track and visualise the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:35:36.249561Z",
     "iopub.status.busy": "2022-04-13T13:35:36.248847Z",
     "iopub.status.idle": "2022-04-13T13:35:36.269978Z",
     "shell.execute_reply": "2022-04-13T13:35:36.268877Z",
     "shell.execute_reply.started": "2022-04-13T13:35:36.249386Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_iterator, criterion, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    for reviews, ratings in tqdm(train_iterator):\n",
    "        ratings = ratings.to(DEVICE)\n",
    "        predictions = model(reviews)\n",
    "        loss = criterion(predictions, ratings)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if scheduler != None:\n",
    "        scheduler.step()\n",
    "\n",
    "def predict(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    predicted_ratings = torch.tensor([], device=DEVICE)\n",
    "    true_ratings = torch.tensor([], device=DEVICE)\n",
    "    losses = torch.tensor([], device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        for reviews, ratings in tqdm(iterator):\n",
    "            ratings = ratings.to(DEVICE)\n",
    "            batch_predictions = model(reviews)\n",
    "            predicted_ratings = torch.cat([predicted_ratings, batch_predictions])\n",
    "            true_ratings = torch.cat([true_ratings, ratings])\n",
    "            batch_losses = criterion(batch_predictions, ratings, reduction='none')\n",
    "            losses = torch.cat([losses, batch_losses])\n",
    "    return losses, predicted_ratings, true_ratings\n",
    "\n",
    "def train(model, train_iterator, criterion, optimizer, n_epochs=10, \n",
    "          val_iterator=None, scheduler=None, project=None, resume=False, params=None):\n",
    "    \n",
    "    # resume is a flag to continue the same wandb run in case we want to continue training\n",
    "    # the model after initial train loop\n",
    "    if resume:\n",
    "        wandb.init(project=project, resume='must', id=wandb.run.id, config=params)\n",
    "    else:\n",
    "        wandb.init(project=project, config=params)\n",
    "        \n",
    "    wandb.watch(model)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        train_one_epoch(model, train_iterator, criterion, optimizer, scheduler)\n",
    "        if val_iterator != None:\n",
    "            val_loss, val_preds, val_ratings = predict(model, val_iterator, criterion)\n",
    "            print(f'Epoch: {epoch+1}, validation {LOSS_NAME}: {val_loss.mean()}')\n",
    "        train_loss, train_preds, train_ratings = predict(model, train_iterator, criterion)\n",
    "        print(f'Epoch: {epoch+1}, train {LOSS_NAME}: {train_loss.mean()}')\n",
    "        \n",
    "        # logging has to be done separately because we can't keep track of steps for wandb\n",
    "        # in case of continuing training after initial train loop\n",
    "        if val_iterator != None:\n",
    "            wandb.log({\n",
    "              f'mean train {LOSS_NAME}': train_loss.mean(),\n",
    "              f'mean val {LOSS_NAME}': val_loss.mean()\n",
    "              }) \n",
    "        else:\n",
    "            wandb.log({\n",
    "              f'mean train {LOSS_NAME}': train_loss.mean()\n",
    "              }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beginning training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-13T13:35:36.272358Z",
     "iopub.status.busy": "2022-04-13T13:35:36.271808Z",
     "iopub.status.idle": "2022-04-13T16:13:47.391040Z",
     "shell.execute_reply": "2022-04-13T16:13:47.390096Z",
     "shell.execute_reply.started": "2022-04-13T13:35:36.272317Z"
    }
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "RESUME PREVIOUS RUN?  no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming previous run: False\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "N EPOCHS?  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mporfiry\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10 epochs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.14 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20220413_133543-2owm6xn0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/porfiry/hotels-reviews/runs/2owm6xn0\" target=\"_blank\">cerulean-brook-171</a></strong> to <a href=\"https://wandb.ai/porfiry/hotels-reviews\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:44<00:00,  9.53it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, validation MAE: 0.8343373537063599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:12<00:00, 16.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train MAE: 0.8318880796432495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:40<00:00,  9.60it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, validation MAE: 0.7672584652900696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:10<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train MAE: 0.7548600435256958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:41<00:00,  9.58it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, validation MAE: 0.7461735010147095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:10<00:00, 16.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, train MAE: 0.7248134016990662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:42<00:00,  9.57it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, validation MAE: 0.789193868637085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:09<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, train MAE: 0.7558698058128357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:42<00:00,  9.57it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, validation MAE: 0.7319957613945007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:10<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, train MAE: 0.6908090710639954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:40<00:00,  9.60it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, validation MAE: 0.730614960193634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:10<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, train MAE: 0.6758401393890381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:40<00:00,  9.60it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, validation MAE: 0.7352918386459351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:09<00:00, 16.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, train MAE: 0.6714523434638977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 2602/5000 [04:29<04:11,  9.53it/s]wandb: Network error (ReadTimeout), entering retry loop.\n",
      "100%|██████████| 5000/5000 [08:39<00:00,  9.62it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, validation MAE: 0.7214157581329346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:10<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, train MAE: 0.6377750039100647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:42<00:00,  9.57it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, validation MAE: 0.717763364315033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:09<00:00, 16.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, train MAE: 0.6236904859542847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [08:41<00:00,  9.59it/s]\n",
      "100%|██████████| 79/79 [01:55<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, validation MAE: 0.7225016951560974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:10<00:00, 16.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, train MAE: 0.6116275787353516\n"
     ]
    }
   ],
   "source": [
    "def log_params(): #log model and training parameters to wandb\n",
    "    params = {}\n",
    "    params['bidirectional'] = BIDIRECTIONAL\n",
    "    params['num layers'] = NUM_LAYERS\n",
    "    params['hidden dim'] = HIDDEN_DIM\n",
    "    params['dropout'] = DROPOUT\n",
    "    params['learning rate'] = LEARNING_RATE\n",
    "    params['train batch size'] = BATCH_SIZES[0]\n",
    "    params['model'] = bert_lstm\n",
    "    params['optimizer'] = optimizer\n",
    "    params['scheduler'] = scheduler\n",
    "    return params\n",
    "\n",
    "def start_training(params):\n",
    "    resume = True if (str.lower(input('RESUME PREVIOUS RUN? ')) == 'y') else False\n",
    "    print('Resuming previous run:', resume)\n",
    "    \n",
    "    n_epochs = int(input('N EPOCHS? '))\n",
    "    print('Training', n_epochs, 'epochs', end='\\n')\n",
    "\n",
    "    train(bert_lstm, \n",
    "          train_dataloader, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          n_epochs=n_epochs,\n",
    "          val_iterator=val_dataloader,\n",
    "          scheduler=scheduler,\n",
    "          project='hotels-reviews', \n",
    "          resume=resume, \n",
    "          params=params)\n",
    "\n",
    "params = log_params()\n",
    "start_training(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result minor improvement over GloVe embeddings has been achieved, but at the cost of a very high computational and time complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
